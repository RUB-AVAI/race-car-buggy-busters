\textbf{TODO: Incorporate elements of the presentation into the text.} The whole implementation can be found in the GitHub 
\textbf{TODO: Link to repository!} repository. In this section we will show the purposes of each node implementation and how they are related to achieve the desired functionalities.


The implementation timeline was mostly usecase driven since our team did not have any practical experience with the ROS2 middleware and Gazebo Fortress as a simulator. Because of this we decided early on that we need a way to iterate over different implementations and test sensor feedback from the car platform. In order to get familiar with the ROS2 message formats and the implementation of ROS2 nodes in python using \textit{rclpy} as a dependency we implemented the \textit{wasd\_control} node. The node is located in the \textit{test\_package} of our ROS2 workspace. The package is utilized as a point in the project where ideas could be implemented and integrated quickly without interrupting the main stack. The control node acts as a keyboard controller for the ackermann steering system and gave us some insights of how the system needs to be updated in order to get a smooth driving behavior. The node implements a state machine that checks for keyboardThis was also the point in the project where we did face the delays that are introduced by the bad network connection available for the project. We experienced control delays from our machine to the vehicle of up to one second which is infeasible for a live feedback loop. This also limited our capabilities of creating visualizations since we had no way of sending them to our monitors since the network bandwidth was so low.


Because of this limitation and the only local availability of the vehicle we moved to implementing the simulation using Gazebo Fortress. In order to work with Gazebo Fortress we had to familiarize ourselves with the concept of modeling for a simulation and in what file format we want to define our models. Ultimately we chose the SDF file format since it is an open source format with relatively new and comprehensive documentation. The URDF and XACRO formats are mainly associated with ROS in the documentation which introduces a barrier, making it harder to work with. These file formats also separate aspects of the same model into different files. This reduces observability of connected components. Having everything in a single file format also reduces complexity since we only had to learn a single new format. In order to implement the vehicle platform and a world around it we heavily utilized the tutorials on the Gazebo documentation \cite{gazebo-documentation}. The first iteration of the vehicle platform was implemented using their \textit{Build your own robot} tutorial and later remodeled once we had a deeper understanding about the file formats and available plugins. At this stage in the development we were able to utilize the examples implemented in the \textit{gazebo\_tutorials} GitHub repository \cite{gazebo-tutorials}. From the repository we learned how to model the Gazebo UI using SDF files and how to implement the transform tree for our vehicle platform in order to have a better transfer to our ROS2 implementation. In the later stages of developing the simulation we focused on getting the data and structure of the messages as close to the real vehicle platform as possible. This was achieved using the ROS2-Gazebo-Bridge \cite{ros-gz-bridge}. With this bridge we could convert the Gazebo-messages to ROS2 messages and forward the topics to ROS2 as well as remap them to the correct paths if necessary. In the end the simulation was close to interchangeable with the implementation of the car platform and we could reuse our complete software stack in the launch files for Gazebo without heavy modifications.


In order to allow control on the platform car we have implemented a \textbf{WASDControl} node that allows manual control of an autonomous vehicle using the WASD keys to send Ackermann steering commands. It listens for keyboard input, translates it into driving commands, and publishes them to the \texttt{/drive} topic.
The \textbf{InitDrive} is responsible for publishing an initial driving command to the \texttt{/drive} topic when it starts. It sends an AckermannDriveStamped message with a predefined speed and no steering angle, while the \textbf{M2P (Move to Point)} node is responsible for navigating the vehicle towards a given target point by processing odometry updates. It listens for target points, calculates the necessary steering and speed, and then sends commands to the \texttt{/drive} topic.\\
\newline
For the cone detection, we have implemented a \textbf{YoloConeDetectionNode} that detects cones in a camera image using a YOLO (You Only Look Once) deep learning model. It processes synchronized RGB images, camera info, and depth images, estimates the 3D positions of detected cones, and publishes the results as DetectedConeArray messages to the \texttt{/yolo\_cones} topic.
The detected cones will be visualized in RViz due to \textbf{ConeMarkerNode} that listens to a DetectedConeArray, transforms the cone positions into a target frame, and publishes visual markers to RViz.\\
\newline
In order to create an occupancy map a \textbf{SemanticGridVisualizerNode} was created that build a semantic map by integrating detected cones into a SLAM-generated occupancy grid. It processes both SLAM maps and detected cones, filters large objects and assigns semantic labels to obstacles.
The \textbf{SemanticGridVisualizerNode} is responsible for visualizing cone clusters detected in a semantic grid using RViz. It processes the semantic occupancy grid, clusters detected cones based on their labels, computes the centroid of each cluster, and publishes visual markers to be displayed in RViz.\\
\newline
Having a map allows us to begin exploring by creating an \textbf{ExplorationNode} that generates target points for an autonomous vehicle by analysing cone positions from the semantic grid. It computes a new driving target point based on the vehicle's position and orientation from the \texttt{/pose} topic, detected cones from the semantic grid, finding the closest blue cone (left) and yellow cone (right) and computing their midpoint as the next driving target. It also monitors steering angle and allows activating/deactivating exploration.
These target points are visualized by the \textbf{ExplorationVisualizerNode} that listens to target point messages and publishes a marker to make the points visible in RViz.\\
\newline
Also for the path planning a \textbf{GlobalPlanningNode} was developed that is responsible for creating an optimized global path for the vehicle to follow after it has completed an exploratory lap. It transitions from exploration mode to global path execution, to ensure that the vehicle follows an optimal trajectory.
