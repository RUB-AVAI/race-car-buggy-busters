\textbf{TODO: Incorporate elements of the presentation into the text.} The whole implementation can be found in the GitHub 
\textbf{TODO: Link to repository!} repository. In this section we will show the purposes of each node implementation and how they are related to achieve the desired functionalities.
In order to allow control on the platform car we have implemented a \textbf{WASDControl} node that allows manual control of an autonomous vehicle using the WASD keys to send Ackermann steering commands. It listens for keyboard input, translates it into driving commands, and publishes them to the \texttt{/drive} topic.
The \textbf{InitDrive} is responsible for publishing an initial driving command to the \texttt{/drive} topic when it starts. It sends an AckermannDriveStamped message with a predefined speed and no steering angle, while the \textbf{M2P (Move to Point)} node is responsible for navigating the vehicle towards a given target point by processing odometry updates. It listens for target points, calculates the necessary steering and speed, and then sends commands to the \texttt{/drive} topic.\\
\newline
For the cone detection, we have implemented a \textbf{YoloConeDetectionNode} that detects cones in a camera image using a YOLO (You Only Look Once) deep learning model. It processes synchronized RGB images, camera info, and depth images, estimates the 3D positions of detected cones, and publishes the results as DetectedConeArray messages to the \texttt{/yolo\_cones} topic.
The detected cones will be visualized in RViz due to \textbf{ConeMarkerNode} that listens to a DetectedConeArray, transforms the cone positions into a target frame, and publishes visual markers to RViz.\\
\newline
In order to create an occupancy map a \textbf{SemanticGridVisualizerNode} was created that build a semantic map by integrating detected cones into a SLAM-generated occupancy grid. It processes both SLAM maps and detected cones, filters large objects and assigns semantic labels to obstacles.
The \textbf{SemanticGridVisualizerNode} is responsible for visualizing cone clusters detected in a semantic grid using RViz. It processes the semantic occupancy grid, clusters detected cones based on their labels, computes the centroid of each cluster, and publishes visual markers to be displayed in RViz.\\
\newline
Having a map allows us to begin exploring by creating an \textbf{ExplorationNode} that generates target points for an autonomous vehicle by analysing cone positions from the semantic grid. It computes a new driving target point based on the vehicle's position and orientation from the \texttt{/pose} topic, detected cones from the semantic grid, finding the closest blue cone (left) and yellow cone (right) and computing their midpoint as the next driving target. It also monitors steering angle and allows activating/deactivating exploration.
These target points are visualized by the \textbf{ExplorationVisualizerNode} that listens to target point messages and publishes a marker to make the points visible in RViz.\\
\newline
Also for the path planning a \textbf{GlobalPlanningNode} was developed that is responsible for creating an optimized global path for the vehicle to follow after it has completed an exploratory lap. It transitions from exploration mode to global path execution, to ensure that the vehicle follows an optimal trajectory.